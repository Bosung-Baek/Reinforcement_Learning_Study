{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = Dense(24, activation='relu')\n",
    "        self.fc2 = Dense(24, activation='relu')\n",
    "        self.fc_out = Dense(action_size,\n",
    "                            kernel_initializer=RandomUniform(-1e-3, 1e-3))\n",
    "\n",
    "    def call(self, x):\n",
    "        # x = tf.expand_dims(x, axis=0)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   0 | score avg 6.01 | memory length:  200 | epsilon: 1.0000\n",
      "episode:   1 | score avg 7.01 | memory length:  400 | epsilon: 1.0000\n",
      "episode:   2 | score avg 6.83 | memory length:  600 | epsilon: 1.0000\n",
      "episode:   3 | score avg 7.94 | memory length:  800 | epsilon: 1.0000\n",
      "episode:   4 | score avg 8.11 | memory length: 1000 | epsilon: 0.9990\n",
      "episode:   5 | score avg 8.15 | memory length: 1200 | epsilon: 0.8178\n",
      "episode:   6 | score avg 8.40 | memory length: 1400 | epsilon: 0.6695\n",
      "episode:   7 | score avg 10.13 | memory length: 1600 | epsilon: 0.5481\n",
      "episode:   8 | score avg 10.26 | memory length: 1800 | epsilon: 0.4487\n",
      "episode:   9 | score avg 10.51 | memory length: 2000 | epsilon: 0.3673\n",
      "episode:  10 | score avg 11.31 | memory length: 2000 | epsilon: 0.3007\n",
      "episode:  11 | score avg 12.76 | memory length: 2000 | epsilon: 0.2462\n",
      "episode:  12 | score avg 13.84 | memory length: 2000 | epsilon: 0.2015\n",
      "episode:  13 | score avg 12.92 | memory length: 2000 | epsilon: 0.1650\n",
      "episode:  14 | score avg 14.26 | memory length: 2000 | epsilon: 0.1351\n",
      "episode:  15 | score avg 15.67 | memory length: 2000 | epsilon: 0.1106\n",
      "episode:  16 | score avg 17.06 | memory length: 2000 | epsilon: 0.0905\n",
      "episode:  17 | score avg 18.24 | memory length: 2000 | epsilon: 0.0741\n",
      "episode:  18 | score avg 19.49 | memory length: 2000 | epsilon: 0.0607\n",
      "episode:  19 | score avg 21.30 | memory length: 2000 | epsilon: 0.0497\n",
      "episode:  20 | score avg 22.54 | memory length: 2000 | epsilon: 0.0407\n",
      "episode:  21 | score avg 22.38 | memory length: 2000 | epsilon: 0.0333\n",
      "episode:  22 | score avg 25.83 | memory length: 2000 | epsilon: 0.0272\n",
      "episode:  23 | score avg 26.53 | memory length: 2000 | epsilon: 0.0223\n",
      "episode:  24 | score avg 28.37 | memory length: 2000 | epsilon: 0.0183\n",
      "episode:  25 | score avg 27.65 | memory length: 2000 | epsilon: 0.0149\n",
      "episode:  26 | score avg 27.07 | memory length: 2000 | epsilon: 0.0122\n",
      "episode:  27 | score avg 29.71 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  28 | score avg 28.99 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  29 | score avg 30.06 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  30 | score avg 30.31 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  31 | score avg 32.82 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  32 | score avg 31.72 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  33 | score avg 30.70 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  34 | score avg 34.02 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  35 | score avg 37.63 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  36 | score avg 41.89 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  37 | score avg 46.94 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  38 | score avg 50.74 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  39 | score avg 52.98 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  40 | score avg 51.89 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  41 | score avg 54.88 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  42 | score avg 54.97 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  43 | score avg 56.98 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  44 | score avg 59.66 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  45 | score avg 56.91 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  46 | score avg 59.58 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  47 | score avg 62.69 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  48 | score avg 59.65 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  49 | score avg 56.76 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  50 | score avg 60.75 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  51 | score avg 58.88 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  52 | score avg 63.11 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  53 | score avg 63.49 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  54 | score avg 62.52 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  55 | score avg 58.47 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  56 | score avg 59.37 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  57 | score avg 56.22 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  58 | score avg 52.71 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  59 | score avg 52.23 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  60 | score avg 58.04 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  61 | score avg 61.42 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  62 | score avg 66.98 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  63 | score avg 65.42 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  64 | score avg 68.98 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  65 | score avg 65.05 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  66 | score avg 68.09 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  67 | score avg 71.20 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  68 | score avg 73.87 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  69 | score avg 75.91 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  70 | score avg 75.83 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  71 | score avg 78.40 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  72 | score avg 80.51 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  73 | score avg 80.29 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  74 | score avg 74.85 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  75 | score avg 74.75 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  76 | score avg 75.33 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  77 | score avg 70.52 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  78 | score avg 70.41 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  79 | score avg 68.50 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  80 | score avg 74.41 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  81 | score avg 78.87 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  82 | score avg 74.71 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  83 | score avg 79.54 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  84 | score avg 75.07 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  85 | score avg 76.44 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  86 | score avg 81.91 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  87 | score avg 85.42 | memory length: 2000 | epsilon: 0.0100\n",
      "episode:  88 | score avg 91.67 | memory length: 2000 | epsilon: 0.0100\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\layen\\anaconda3\\envs\\cuda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.render = True\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = DQN(action_size)\n",
    "        self.target_model = DQN(action_size)\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.update_target_model()\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model(state)\n",
    "            return np.argmax(q_value[0])\n",
    "        \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        # print(state)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        # print(self.memory)\n",
    "        # print(self.memory[0])\n",
    "        \n",
    "        # states = np.array([sample[0][0] if isinstance(sample[0][0], float) else sample[0][0][0] if isinstance(sample[0][0], np.ndarray) else sample[0] if isinstance(sample[0], float) else sample[0][0] for sample in mini_batch])\n",
    "\n",
    "        states = np.array([sample[0][0] for sample in mini_batch])\n",
    "        # print(states)\n",
    "        actions = np.array([sample[1] for sample in mini_batch])\n",
    "        rewards = np.array([sample[2] for sample in mini_batch])\n",
    "        next_states = np.array([sample[3][0] for sample in mini_batch])\n",
    "        dones = np.array([sample[4] for sample in mini_batch])\n",
    "        \n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # states = tf.reshape(states, [-1, 1])\n",
    "            # print(states)\n",
    "            predicts = self.model(states)\n",
    "            \n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            # print(one_hot_action)\n",
    "            # # print(predicts)\n",
    "            # predicts = np.squeeze(predicts, axis=0)\n",
    "            # print(one_hot_action)\n",
    "            # print(\"predicts\",  predicts.shape)\n",
    "            predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)            \n",
    "            \n",
    "            target_predicts = self.target_model(next_states)\n",
    "            target_predicts = tf.stop_gradient(target_predicts)\n",
    "            \n",
    "            max_q = np.amax(target_predicts, axis=-1)\n",
    "            targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "            # print(\"targets\",  targets.shape)\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.square(targets - predicts))\n",
    "            # print(\"loss\", loss)\n",
    "            \n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCar-v0', render_mode='human')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    # print(env.observation_space)\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    agent = Agent(state_size, action_size)\n",
    "    \n",
    "    scores, episodes = [], []\n",
    "    score_avg = 0\n",
    "    \n",
    "    num_episode = 1000\n",
    "    for e in range(num_episode):\n",
    "        done= False\n",
    "        score = 0\n",
    "        state = env.reset()[0]\n",
    "        \n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "            if agent.render and e % 10 == 0:\n",
    "                env.render()\n",
    "                \n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, _, done, _ = env.step(action)\n",
    "            \n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "\n",
    "            # 수정된 보상 계산\n",
    "            if done:\n",
    "                if next_state[0, 0] >= 0.5:\n",
    "                    reward = 0.1  # 목표 지점에 도달한 경우\n",
    "                else:\n",
    "                    reward = -1  # 실패한 경우\n",
    "            else:\n",
    "                reward = np.abs(next_state[0, 0] - (0.5))\n",
    "                \n",
    "            score += reward\n",
    "            \n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {:3d} | score avg {:3.2f} | memory length: {:4d} | epsilon: {:.4f}\".format(\n",
    "                    e, score_avg, len(agent.memory), agent.epsilon))\n",
    "\n",
    "\n",
    "                \n",
    "                if score_avg > 90:\n",
    "                    \n",
    "                    sys.exit()\n",
    "                    \n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
