{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    def __init__(self):\n",
    "        # 다른 객체 변수 필요없음!\n",
    "        self.q_table = np.zeros((5, 5, 4))\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def update(self):\n",
    "        # update를 할 때마다 식 3.20에 따라 q_table의 값을 변경\n",
    "        gamma = self.gamma\n",
    "        q_table = self.q_table\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                for action in range(4):\n",
    "                    r, c, reward = self.get_next_state((row, col), action)\n",
    "                    q_table[row, col, action] = reward + gamma * np.max(q_table[r, c, :])\n",
    "        self.q_table = q_table\n",
    "        \n",
    "\n",
    "    def get_next_state(self, state:tuple[int, int], action:int)->tuple[int, int, int]:\n",
    "        # update 함수에 사용할 state와 action을 넣으면 next state와 reward를 반환하는 함수\n",
    "        if state == (0, 1):\n",
    "            row, col = (4, 1)\n",
    "            reward = 10\n",
    "            return (row, col, reward)\n",
    "        elif state == (0, 3):\n",
    "            row, col = (2, 3)\n",
    "            reward = 5\n",
    "            return (row, col, reward)\n",
    "        else:\n",
    "            try:\n",
    "                if action == 0:\n",
    "                    row, col = state[0]-1, state[1]\n",
    "                elif action == 1:\n",
    "                    row, col = state[0]+1, state[1]\n",
    "                elif action == 2:\n",
    "                    row, col = state[0], state[1]-1\n",
    "                else:\n",
    "                    row, col = state[0], state[1]+1\n",
    "                value = self.q_table[row, col, :]\n",
    "                reward = 0\n",
    "            except IndexError:\n",
    "                row, col = state[0], state[1]\n",
    "                reward = -1 \n",
    "            return (row, col, reward)\n",
    "        \n",
    "    def get_policy(self):\n",
    "        # q_table 값에 따라 정책 화살표로 print\n",
    "        dir_dict = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
    "        direction_list = []\n",
    "        for row in range(5):\n",
    "            direction_list_row = []\n",
    "            for col in range(5):\n",
    "                if row == 0 and col == 1 or row == 0 and col == 3:\n",
    "                    direction_list_row.append(\"* \")\n",
    "                    continue\n",
    "                action_value = \"\"\n",
    "                \n",
    "                # 최대값을 가지는 행동의 모든 인덱스를 가져옴\n",
    "                max_idx_list = np.argwhere(self.q_table[row, col, :] == np.max(self.q_table[row, col, :])).flatten().tolist()\n",
    "\n",
    "                for i in max_idx_list:\n",
    "                    action_value += dir_dict[i]\n",
    "                action_value += \" \" * (2-len(max_idx_list))\n",
    "\n",
    "                direction_list_row.append(action_value)\n",
    "\n",
    "            direction_list.append(direction_list_row)\n",
    "        return direction_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 상태 가치 함수\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "최적 행동 가치 함수로 계산한 최적 상태 가치 함수  (100번 반복)\n",
      "[[21.97748529 24.4194281  21.97748529 19.4194281  17.47748529]\n",
      " [19.77973676 21.97748529 19.77973676 17.80176308 16.02158677]\n",
      " [17.80176308 19.77973676 17.80176308 16.02158677 14.4194281 ]\n",
      " [16.02158677 17.80176308 16.02158677 14.4194281  12.97748529]\n",
      " [14.4194281  16.02158677 14.4194281  12.97748529 11.67973676]]\n",
      "\n",
      "최적 행동 가치 함수로 계산한 정책\n",
      "[['→ ' '* ' '← ' '* ' '← ']\n",
      " ['↑→' '↑ ' '↑←' '← ' '← ']\n",
      " ['↑→' '↑ ' '↑←' '↑←' '↑←']\n",
      " ['↑→' '↑ ' '↑←' '↑←' '↑←']\n",
      " ['↑→' '↑ ' '↑←' '↑←' '↑←']]\n"
     ]
    }
   ],
   "source": [
    "env = Environment2()\n",
    "print(\"초기 상태 가치 함수\")\n",
    "print(np.max(env.q_table, axis=2))\n",
    "\n",
    "for i in range(100):\n",
    "    env.update()\n",
    "\n",
    "print(\"\\n최적 행동 가치 함수로 계산한 최적 상태 가치 함수  (100번 반복)\")\n",
    "print(np.max(env.q_table, axis=2))\n",
    "\n",
    "print(\"\\n최적 행동 가치 함수로 계산한 정책\")\n",
    "print(np.array(env.get_policy()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
